<html>

<head>
	<meta http-equiv="Content-Type" content="text/html;charset=utf-8">
	<title>Analysis report</title>
</head>

<body>
	<h1 id="top">Results</h1>

	<table border="1" cellspacing="0" cellpadding="5">
		<tr>
			<th rowspan=2>Algorithms</th>
			<th colspan=4>Estimated number of clusters</th>
		</th>
		<tr>
			<th>Gap statistic</th>
			<th>Silhouette score</th>
			<th>Davies-Bouldin score</th>
			<th>Calinski and Harabasz score</th>
		</tr>

	{%- for algorithm in algorithms %}
		<tr>
			<td><a href="#{{ algorithm.name }}">{{ algorithm.name }}</a></td>
			<td>{{ algorithm.results['gap_nc'] }}</td>
			<td>{{ algorithm.results['silhouette_nc'] }}</td>
			<td>{{ algorithm.results['davies_nc'] }}</td>
			<td>{{ algorithm.results['calinski_nc'] }}</td>
		</tr>
	{%- endfor %}
	</table>
	<p>
	According to the majority rule, the estimated number of clusters is <strong>{{ nc }}</strong>.
	</p>
	<br>

	<h2>Clustering <font size="-1">[<a href="#top">Back To Top</a>]</font></h2>
	<p>
		<ul>
			<li>Clustering is the task of <strong>grouping a set of data samples in such a way that the data samples in the same group are more similar to each other</strong> than to those in the other groups.</li>
			<li>In general, data samples have no labels, so we <strong>need to interpret the clustering results</strong>.</br>
			    This type of problem is called <strong>unsupervised learning</strong>.
			</li>
			<li>Many clustering algorithms <strong>require the user to set the number of clusters</strong> in advance.</br>
			    However, the optimal number of clusters is unknown.</br>
				Therefore, MALSS conducts the cluster analysis with a range of candidates for the number of clusters.
			</li>
			<li>A wide variety of indices have been proposed to find the optimal number of clusters.</br>
			    MALSS <strong>estimates the optimal number of clusters by majority vote</strong> of these indices.
			</li>
			<li>The indices that MALSS uses are as follows: 
			    <a href="https://statweb.stanford.edu/~gwalther/gap" target="new">Gap statistic</a>,
				<a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html" target="new">Silhouette score</a>,
				<a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html" target="new">Davies-Bouldin score</a>,
				<a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html" target="new">Calinski and Harabasz score</a>.
			</li>
		</ul>
	</p>
	<hr>

	<h2>Data summary <font size="-1">[<a href="#top">Back To Top</a>]</font></h2>
	<ul>
		<li>Number of rows: {{ data.shape_before[0] }}</li>
		<li>Number of columns: {{ data.shape_before[1] }} (numerical: {{ data.shape_before[1] - data.del_columns|length }}, categorical: {{ data.del_columns|length }})</li>
		{%- if data.del_columns|length > 0 %}
		<ul>
			<li>Categorical values were encoded to integer features using a <a href="http://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features">one-of-K scheme</a>.</li>
		</ul>
		{%- endif %}
		{%- if data.col_was_null|length > 0 %}
		<li>Column {% for col in data.col_was_null %}{{ col }} {% endfor %}had NA values.</li>
		<ul>
			<li>NA values were filled in with the most frequent value (categorical), median (integer), mean (float).</li>
			<li>See also: <a href="http://pandas.pydata.org/pandas-docs/stable/missing_data.html">missing data</a></li>
		</ul>
		{%- endif %}
	</ul>
	<details>
	<summary><font size="+1"><strong>Descriptions (click here)</strong></font></summary>
	<ul>
		<li>Clustering algorithms are affected by the difference of the scale of each attribute.</br>
		    <strong>If the scale of an attribute is much smaller than that of the other attributes</strong> (e.g. the attribute has one digit and the other attributes have five digits.), <strong>the effect of the attribute is ignored</strong>.</br>
			Therefore, <strong>MALSS standardizes the data</strong> to a mean of zero and standard deviation of one by default.</br>
			If scaling is not needed, set the <i>standardize</i> parameter to <i>True</i>.
		</li>
		<li>Note that some features (e.g. country code) may have to be handled as the categorical feature even though they look like numerical features.</br>
		    In such a case, you need to encode categorical features into numerical features by yourself before setting data to MALSS.</br>
			<a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html" target="new"><strong>One-hot encoding</strong></a> is commonly used.
		</li>
		<li><strong>Clustering algorithms work poorly as the number of features increases</strong>, which is known as the <a href="https://en.wikipedia.org/wiki/Clustering_high-dimensional_data" target="new">curse of dimensionality</a>.</br>
			One-hot encoding sometimes causes this problem.</br>
			<a href="https://en.wikipedia.org/wiki/Feature_selection" target="new">Feature selection</a> or <a href="https://en.wikipedia.org/wiki/Dimensionality_reduction" target="new">dimensionality reduction</a> may help in such a case.
		</li>
	</ul>
	</details>
	<hr>

	<h2 id="{{ algorithms[0].name }}"><a href="{{ algorithms[0].link }}">K-means clustering</a> <font size="-1">[<a href="#top">Back To Top</a>]</font></h2>
	<p>
		<ul>
			<li><a href="https://en.wikipedia.org/wiki/K-means_clustering" target="new">K-means clustering</a> is one of the most commonly used clustering algorithms.</li>
			<li>Note that the <strong>k-means algorithm assumes that clusters are spherical and of equal size</strong>.</br>
			    If clusters are not spherical or not of equal size, the k-means algorithm may produce undesirable results (see the figure below).
			</li>
			<img border="0" src="kmeans_mouse.png" height="300" alt="kmeans_mouse">
		</ul>
	</p>

	<h3>Results of estimating the number of clusters</h3>
	<h4>Gap statistics</h4>
	<img border="0" src="gap_{{ algorithms[0].estimator.__class__.__name__ }}.png" height="300" alt="gap_statistics">

	<h4>Silhouette score</h4>
	<img border="0" src="silhouette_{{ algorithms[0].estimator.__class__.__name__ }}.png" height="300" alt="silhouette_score">
    
	<h4>Davies-Bouldin score</h4>
	<img border="0" src="davies_{{ algorithms[0].estimator.__class__.__name__ }}.png" height="300" alt="davies_bouldin_score">
    
	<h4>Calinski and Harabasz score</h4>
	<img border="0" src="calinski_{{ algorithms[0].estimator.__class__.__name__ }}.png" height="300" alt="calinski_harabasz_score">
    
	<h2 id="{{ algorithms[1].name }}"><a href="{{ algorithms[1].link }}">Hierarchical clustering</a> <font size="-1">[<a href="#top">Back To Top</a>]</font></h2>
	<p>
		<ul>
			<li>The <a href="https://en.wikipedia.org/wiki/Hierarchical_clustering" target="new">hierarchical clustering</a> (like the k-means algorithm) is one of the most commonly used clustering algorithms.</li>
			<li>Hierarchical clustering algorithms are broadly divided into two groups.</br>
			    One is the <strong>agglomerative (bottom-up) approach</strong> where each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.</br>
				The other is the <strong>divisive (top-down) approach</strong> where all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.</br>
				MALSS supports agglomerative clustering methods.
			</li>
			<li>Hierarchical clustering algorithms have an advantage that they can visualize the results of clustering as a cluster tree called a <strong>dendrogram</strong>.</br>
			    Note the following points when referring to the dendrogram:
				<ul>
					<li><strong>Proximity along the horizontal axis of the dendrogram doesn't represent the similarity of two observations</strong>.</br>
					    We need to see the location on the vertical axis where branches containing those two observations first are fused.
					</li>
					<li><strong>The assumption that an arbitrary data has hierarchical structure might be unrealistic</strong> though the results of hierarchical clustering always have hierarchical structures.</li>
					<li>A dendrogram strongly depends on the type of <a href="https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.linkage.html" target="new">linkage</a>, which defines the dissimilarity between two groups of observations.</br>
					    MALSS adopts a <i>complete linkage method</i>.
					</li>
				</ul>
			</li>
		</ul>
	</p>

	<h3>Dendrogram</h3>
	<img border="0" src="dendrogram_{{ algorithms[1].estimator.__class__.__name__ }}.png" height="400" alt="dendrogram">

	<h3>Results of estimating the number of clusters</h3>
	<h4>Gap statistics</h4>
	<img border="0" src="gap_{{ algorithms[1].estimator.__class__.__name__ }}.png" height="300" alt="gap_statistics">

	<h4>Silhouette score</h4>
	<img border="0" src="silhouette_{{ algorithms[1].estimator.__class__.__name__ }}.png" height="300" alt="silhouette_score">
    
	<h4>Davies-Bouldin score</h4>
	<img border="0" src="davies_{{ algorithms[1].estimator.__class__.__name__ }}.png" height="300" alt="davies_bouldin_score">
    
	<h4>Calinski and Harabasz score</h4>
	<img border="0" src="calinski_{{ algorithms[1].estimator.__class__.__name__ }}.png" height="300" alt="calinski_harabasz_score">
    
</body>
</html>
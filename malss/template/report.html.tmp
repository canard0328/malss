<html>

<head>
	<meta http-equiv="Content-Type" content="text/html;charset=utf-8">
	<title>Analysis report</title>
</head>

<body>
	<h1 id="top">Results</h1>

	<table border="1" cellspacing="0" cellpadding="5">
		<tr>
			<th>algorithm</th>
			<th>cross-validation score ({{ scoring }})</th>
		</tr>

	{%- for algorithm in algorithms %}
		<tr>
			<td><a href="#{{ algorithm.name }}">{{ algorithm.name }}</a></td>
			<td><font color="{% if algorithm.is_best_algorithm %}#FF0000{% else %}#000000{% endif %}">{{ algorithm.best_score|round(5) }}</font></td>
		</tr>
	{%- endfor %}
	</table>

	{%- if verbose %}
	<p>
		<strong>* cross-vaidation score:</strong>
		<ul>
			<li>Evaluating the quality of the model on the data used to fit the model can lead to <a href="http://en.wikipedia.org/wiki/Overfitting"><i>overfitting</i></a>.</li>
			<li>The solution to overfitting is <a href="http://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation"><i>k-fold cross validation</i></a>: divide the dataset into k (default: 5) parts of (roughly) equal size, then for each of these five parts, train the classifier on the other four and test on the held-out part.</li>
		</ul>
	</p>
	{%- endif %}
	<hr>

	{%- for algorithm in algorithms %}
	<h2 id="{{ algorithm.name }}">{{ algorithm.name }} <font size="-1">[<a href="#top">Back To Top</a>]</font></h2>
	<h3>Parameter optimization by <a href="http://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search"><i>grid search</i></a></h3>
	<table border="1" cellspacing="0" cellpadding="5">
		<tr>
		{%- for key in algorithm.grid_scores[0][0].keys() %}
			<th>{{ key }}</th>
		{%- endfor %}
			<th>{{ scoring }}</th>
			<th>SD</th>
		</tr>
		{%- for scr in algorithm.grid_scores %}
		<tr>
			{%- for val in scr[0].values() %}
			<td><font color="{% if scr[0] == algorithm.best_params %}#FF0000{% else %}#000000{% endif %}">{{ val }}</font></td>
			{%- endfor %}
			<td><font color="{% if scr[0] == algorithm.best_params %}#FF0000{% else %}#000000{% endif %}">{{ scr[1]|round(3) }}</font></td>
			<td><font color="{% if scr[0] == algorithm.best_params %}#FF0000{% else %}#000000{% endif %}">{{ scr[2].std()|round(3) }}</font></td>
		</tr>
		{%- endfor %}
	</table>

	{%- if verbose %}
	<p>
		<ul>
			<li>If the best parameter is at the border of the grid, its range should be expanded.</li>
			<li>Often a second, narrower grid is searched centered around the best parameters of the first grid.</li>
		</ul>
	</p>
	{%- endif %}

	{%- if task == "classification" %}
	<h3>Classification report</h3>
	<pre>{{ algorithm.classification_report }}</pre>
	{%- endif %}

	<h3>Learning curve</h3>
	<img border="0" src="learning_curve_{{ algorithm.estimator.__class__.__name__ }}.png" height="300" alt="learning_curve">

	{%- if verbose %}
	<p>
		Learning curve
		<ul>
			<li>A learning curve is a plot of the training and cross-validation error as a function of the number of training points.</li>
			<li>The algorithm shows <strong>high variance</strong> (over-fitting) if:</li>
			<ul>
				<li>Cross-validation score still increasing as training examples increases.</li>
				<li>Large gap between training and cross-validation score.</li>
			</ul>
			<li>The algorithm shows <strong>high bias</strong> (under-fitting) if:</li>
			<ul>
				<li>Even training score is unacceptably low.</li>
				<li>Small gap between training and cross-validation score.</li>
			</ul>
		</ul>
		In case of <strong>high variance</strong> (over-fitting):
		<ul>
			<li>Use fewer features. Using a feature selection technique may be useful, and decrease the over-fitting of the estimator.</li>
			<li>Use more training samples. Adding training samples can reduce the effect of over-fitting, and lead to improvements in a high variance estimator.</li>
		</ul>
		In case of <strong>high bias</strong> (under-fitting):
		<ul>
			<li>Add more features. In our example of predicting home prices, it may be helpful to make use of information such as the neighborhood the house is in, the year the house was built, the size of the lot, etc. Adding these features to the training and test sets can improve a high-bias estimator.</li>
			<li>Use a more sophisticated model. Adding complexity to the model can help improve on bias. For a polynomial fit, this can be accomplished by increasing the degree d. Each learning technique has its own methods of adding complexity.</li>
			<li>Use fewer samples. Though this will not improve the classification, a high-bias algorithm can attain nearly the same error with a smaller training sample. For algorithms which are computationally expensive, reducing the training sample size can lead to very large improvements in speed.</li>
		</ul>
		See also:
		<ul>
			<li><a href="http://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">Bias-variance tradeoff</a></li>
		</ul>
	</p>
	{%- endif %}

	<hr>
	{%- endfor %}
	{%- if verbose %}
	* much of the material in this report was adapted from <a href="http://www.astroml.org/sklearn_tutorial/practical.html">sklearn tutorials</a>.
	{%- endif %}
</body>
</html>
